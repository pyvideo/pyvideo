{
    "added": "2013-07-04T10:08:58",
    "category": "SciPy 2013",
    "copyright_text": "http://www.youtube.com/t/terms",
    "description": "",
    "duration": null,
    "embed": "<object width=\"640\" height=\"390\"><param name=\"movie\" value=\"http://youtube.com/v/Mp1xnPfE4PY?version=3&amp;hl=en_US\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://youtube.com/v/Mp1xnPfE4PY?version=3&amp;hl=en_US\" type=\"application/x-shockwave-flash\" width=\"640\" height=\"390\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed></object>",
    "id": 2125,
    "language": "English",
    "quality_notes": "",
    "recorded": "2013-07-01",
    "related_urls": [],
    "slug": "hyperopt-a-python-library-for-optimizing-machine-1",
    "source_url": "http://www.youtube.com/watch?v=Mp1xnPfE4PY",
    "speakers": [],
    "state": 1,
    "summary": "Hyperopt: A Python library for optimizing the hyperparameters of machine learning algorithms\n\nAuthors: Bergstra, James, University of Waterloo; Yamins, Dan, Massachusetts Institute of Technology; Cox, David D., Harvard University\n\nTrack: Machine Learning\n\nMost machine learning algorithms have hyperparameters that have a great impact on end-to-end system performance, and adjusting hyperparameters to optimize end-to-end performance can be a daunting task. Hyperparameters come in many varieties--continuous-valued ones with and without bounds, discrete ones that are either ordered or not, and conditional ones that do not even always apply (e.g., the parameters of an optional pre-processing stage)--so conventional continuous and combinatorial optimization algorithms either do not directly apply, or else operate without leveraging structure in the search space. Typically, the optimization of hyperparameters is carried out before-hand by domain experts on unrelated problems, or manually for the problem at hand with the assistance of grid search. However, even random search has been shown to be competitive [1].\n\nBetter hyperparameter optimization algorithms (HOAs) are needed for two reasons:\n\nHOAs formalize the practice of model evaluation, so that benchmarking experiments can be reproduced by different people.\n\nLearning algorithm designers can deliver flexible fully-configurable implementations (of e.g. Deep Learning algorithms) to non-experts, so long as they also provide a corresponding HOA.\n\nHyperopt provides serial and parallelizable HOAs via a Python library [2, 3]. Fundamental to its design is a protocol for communication between (a) the description of a hyperparameter search space, (b) a hyperparameter evaluation function (machine learning system), and (c) a hyperparameter search algorithm. This protocol makes it possible to make generic HOAs (such as the bundled \"TPE\" algorithm) work for a range of specific search problems. Specific machine learning algorithms (or algorithm families) are implemented as hyperopt search spaces in related projects: Deep Belief Networks [4], convolutional vision architectures [5], and scikit-learn classifiers [6]. My presentation will explain what problem hyperopt solves, how to use it, and how it can deliver accurate models from data alone, without operator intervention.",
    "tags": [
        "Tech"
    ],
    "thumbnail_url": "http://i1.ytimg.com/vi/Mp1xnPfE4PY/hqdefault.jpg",
    "title": "Hyperopt: A Python library for optimizing machine learning algorithms; SciPy 2013",
    "updated": "2014-04-08T20:28:26.403",
    "video_flv_download_only": false,
    "video_flv_length": null,
    "video_flv_url": null,
    "video_mp4_download_only": false,
    "video_mp4_length": null,
    "video_mp4_url": null,
    "video_ogv_download_only": false,
    "video_ogv_length": null,
    "video_ogv_url": null,
    "video_webm_download_only": false,
    "video_webm_length": null,
    "video_webm_url": null,
    "whiteboard": "needs editing"
}